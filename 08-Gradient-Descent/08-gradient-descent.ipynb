{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent\n",
    "\n",
    "If you successfully completed the previous notebook, you'll now have a good understanding that we can view the log-likelihood function as a landscape.  Unfortunately, it's the rare problem for which that landscape actually exists in 2-dimensions like our own physical topography.  Indeed, for machine learning this landscape often has thousands or millions of dimensions.  This precludes the possibility of finding optimal solutions through grid searches.\n",
    "\n",
    "The alternative to brute force is gradient descent.  Here we'll take the convention of trying to minimize a so-called *cost function*, which for our purposes is just -1 times the log-posterior we've been working with (for historical reasons, the machine learning literature tends to think of optimization problems in terms of minimizing a cost function, as opposed to maximizing a log-posterior, even though the two viewpoints are equivalen).  For the lobster problem, the cost function to minimize is \n",
    "$$\n",
    "\\mathcal{L}(\\mathbf{w};\\mathbf{X}_{obs},\\mathbf{Y}_{obs}) = -\\sum_{i=1}^m \\left[Y_{obs,i} \\ln \\sigma(\\Phi_i \\mathbf{w}) + (1-Y_{obs,i}) \\ln (1-\\sigma(\\Phi_i \\mathbf{w}))\\right] + \\lambda \\mathbf{w}^T \\mathbf{w}. \n",
    "$$\n",
    "The gradient of this function is \n",
    "$$\n",
    "\\nabla_\\mathbf{w} \\mathcal{L} = -\\sum_{i=1}^m \\left[ (Y_{obs,i} - \\sigma(\\Phi_i \\mathbf{w})) \\Phi_i\\right] + \\lambda\\mathbf{w}^T. \n",
    "$$\n",
    "Gradient descent is implemented by sequentially updating the model parameters as\n",
    "$$\n",
    "\\mathbf{w}_{t+1} = \\mathbf{w}_t - \\eta \\nabla_{\\mathbf{w}} \\mathcal{L}.\n",
    "$$\n",
    "**Return to the lobster problem, and attempt to find the optimal value of $\\mathbf{w}$ using gradient descent.  You'll have to select both a sensible initial guess for $\\mathbf{w}$, as well as a value of $\\eta$: too big, and it'll do some crazy things, too small and you won't get anywhere.  Because you've already solved this problem using grid search, you know the solution that you're trying to achieve.  Plot the trajectory of parameter values over the loss surface that you constructed for the brute force method (this is very helpful for debugging).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
